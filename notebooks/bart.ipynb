{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOGMLaY3Ip9a8XZYG6lXvpR"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2KrULHs1O6S",
        "colab_type": "code",
        "outputId": "eed95d53-6084-4d05-a743-932bf6b8cb02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL76V1uI5AdH",
        "colab_type": "code",
        "outputId": "2bda2c5f-7e1f-448a-dc01-e7b7aa874fcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 10 20:46:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0    31W /  70W |   2463MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkow5j7d5JRy",
        "colab_type": "code",
        "outputId": "d7982d97-7089-4a95-b2c2-e096dc927801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImxqguB_5S9o",
        "colab_type": "code",
        "outputId": "b64be321-3bc7-424f-bd34-a27e030f5e17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bart = torch.hub.load('pytorch/fairseq', 'bart.large')\n",
        "bart.eval()  # disable dropout (or leave in train mode to finetune)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BARTHubInterface(\n",
              "  (model): BARTModel(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): LearnedPositionalEmbedding(1026, 1024, padding_idx=1)\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): MultiheadAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (classification_heads): ModuleDict()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIuuaIKbfUNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in bart.parameters():\n",
        "  p.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QZ2fNu-9JGY",
        "colab_type": "text"
      },
      "source": [
        "## Convert whole dataset to bart embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsVSBsw-9gLQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "f73dad7e-de9f-44bb-b66b-63b16d4a026d"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "PATH = '/content/drive/My Drive/Kaggle/Google QUEST Q&A Labeling/data/base/'\n",
        "OUTPUT_PATH = '/content/drive/My Drive/Kaggle/Google QUEST Q&A Labeling/output/2/'\n",
        "\n",
        "df_train = pd.read_csv(PATH+'train.csv').head(100)\n",
        "df_test = pd.read_csv(PATH+'test.csv')\n",
        "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
        "print('train shape =', df_train.shape)\n",
        "print('test shape =', df_test.shape)\n",
        "\n",
        "output_categories = list(df_train.columns[11:])\n",
        "input_categories = list(df_train.columns[[1,2,5]])\n",
        "print('\\noutput categories:\\n\\t', output_categories)\n",
        "print('\\ninput categories:\\n\\t', input_categories)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape = (100, 41)\n",
            "test shape = (476, 11)\n",
            "\n",
            "output categories:\n",
            "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
            "\n",
            "input categories:\n",
            "\t ['question_title', 'question_body', 'answer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDsZcDq-9jSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "370c2197-2a4c-405e-8a69-d6f4f66bde2d"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qa_id</th>\n",
              "      <th>question_title</th>\n",
              "      <th>question_body</th>\n",
              "      <th>question_user_name</th>\n",
              "      <th>question_user_page</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_user_name</th>\n",
              "      <th>answer_user_page</th>\n",
              "      <th>url</th>\n",
              "      <th>category</th>\n",
              "      <th>host</th>\n",
              "      <th>question_asker_intent_understanding</th>\n",
              "      <th>question_body_critical</th>\n",
              "      <th>question_conversational</th>\n",
              "      <th>question_expect_short_answer</th>\n",
              "      <th>question_fact_seeking</th>\n",
              "      <th>question_has_commonly_accepted_answer</th>\n",
              "      <th>question_interestingness_others</th>\n",
              "      <th>question_interestingness_self</th>\n",
              "      <th>question_multi_intent</th>\n",
              "      <th>question_not_really_a_question</th>\n",
              "      <th>question_opinion_seeking</th>\n",
              "      <th>question_type_choice</th>\n",
              "      <th>question_type_compare</th>\n",
              "      <th>question_type_consequence</th>\n",
              "      <th>question_type_definition</th>\n",
              "      <th>question_type_entity</th>\n",
              "      <th>question_type_instructions</th>\n",
              "      <th>question_type_procedure</th>\n",
              "      <th>question_type_reason_explanation</th>\n",
              "      <th>question_type_spelling</th>\n",
              "      <th>question_well_written</th>\n",
              "      <th>answer_helpful</th>\n",
              "      <th>answer_level_of_information</th>\n",
              "      <th>answer_plausible</th>\n",
              "      <th>answer_relevance</th>\n",
              "      <th>answer_satisfaction</th>\n",
              "      <th>answer_type_instructions</th>\n",
              "      <th>answer_type_procedure</th>\n",
              "      <th>answer_type_reason_explanation</th>\n",
              "      <th>answer_well_written</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>What am I losing when using extension tubes in...</td>\n",
              "      <td>After playing around with macro photography on...</td>\n",
              "      <td>ysap</td>\n",
              "      <td>https://photo.stackexchange.com/users/1024</td>\n",
              "      <td>I just got extension tubes, so here's the skin...</td>\n",
              "      <td>rfusca</td>\n",
              "      <td>https://photo.stackexchange.com/users/1917</td>\n",
              "      <td>http://photo.stackexchange.com/questions/9169/...</td>\n",
              "      <td>LIFE_ARTS</td>\n",
              "      <td>photo.stackexchange.com</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>What is the distinction between a city and a s...</td>\n",
              "      <td>I am trying to understand what kinds of places...</td>\n",
              "      <td>russellpierce</td>\n",
              "      <td>https://rpg.stackexchange.com/users/8774</td>\n",
              "      <td>It might be helpful to look into the definitio...</td>\n",
              "      <td>Erik Schmidt</td>\n",
              "      <td>https://rpg.stackexchange.com/users/1871</td>\n",
              "      <td>http://rpg.stackexchange.com/questions/47820/w...</td>\n",
              "      <td>CULTURE</td>\n",
              "      <td>rpg.stackexchange.com</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Maximum protusion length for through-hole comp...</td>\n",
              "      <td>I'm working on a PCB that has through-hole com...</td>\n",
              "      <td>Joe Baker</td>\n",
              "      <td>https://electronics.stackexchange.com/users/10157</td>\n",
              "      <td>Do you even need grooves?  We make several pro...</td>\n",
              "      <td>Dwayne Reid</td>\n",
              "      <td>https://electronics.stackexchange.com/users/64754</td>\n",
              "      <td>http://electronics.stackexchange.com/questions...</td>\n",
              "      <td>SCIENCE</td>\n",
              "      <td>electronics.stackexchange.com</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.888889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Can an affidavit be used in Beit Din?</td>\n",
              "      <td>An affidavit, from what i understand, is basic...</td>\n",
              "      <td>Scimonster</td>\n",
              "      <td>https://judaism.stackexchange.com/users/5151</td>\n",
              "      <td>Sending an \"affidavit\" it is a dispute between...</td>\n",
              "      <td>Y     e     z</td>\n",
              "      <td>https://judaism.stackexchange.com/users/4794</td>\n",
              "      <td>http://judaism.stackexchange.com/questions/551...</td>\n",
              "      <td>CULTURE</td>\n",
              "      <td>judaism.stackexchange.com</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.888889</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>How do you make a binary image in Photoshop?</td>\n",
              "      <td>I am trying to make a binary image. I want mor...</td>\n",
              "      <td>leigero</td>\n",
              "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
              "      <td>Check out Image Trace in Adobe Illustrator. \\n...</td>\n",
              "      <td>q2ra</td>\n",
              "      <td>https://graphicdesign.stackexchange.com/users/...</td>\n",
              "      <td>http://graphicdesign.stackexchange.com/questio...</td>\n",
              "      <td>LIFE_ARTS</td>\n",
              "      <td>graphicdesign.stackexchange.com</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   qa_id  ... answer_well_written\n",
              "0      0  ...            1.000000\n",
              "1      1  ...            0.888889\n",
              "2      2  ...            0.888889\n",
              "3      3  ...            1.000000\n",
              "4      5  ...            1.000000\n",
              "\n",
              "[5 rows x 41 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BOjamzARAno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXHHvitT9qDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 4\n",
        "EVAL_BATCH_SIZE = 1\n",
        "MAX_TITLE_SIZE = 32\n",
        "MAX_QUESTION_SIZE = 128\n",
        "MAX_ANSWER_SIZE = 128\n",
        "BART_EMBEDDING_SIZE = 1024\n",
        "MAX_TITLE_QUESTION_SIZE = MAX_TITLE_SIZE + MAX_QUESTION_SIZE\n",
        "LABELS_COUNT = len(output_categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPkvwKF0FZJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fairseq.data.data_utils import collate_tokens\n",
        "\n",
        "def extract_features_for_batch(batch_snt, max_length):\n",
        "  batch_encoded = []\n",
        "  trimmed_count = 0\n",
        "  for s in batch_snt:\n",
        "    encoded = bart.encode(s)\n",
        "    if len(encoded) <= max_length:\n",
        "      encoded = torch.cat((encoded, torch.tensor([1]).repeat(max_length - len(encoded))), 0)\n",
        "    else:\n",
        "      encoded = encoded[:max_length]\n",
        "      trimmed_count += 1\n",
        "    batch_encoded.append(encoded)\n",
        "  batch = collate_tokens(batch_encoded, pad_idx=1)\n",
        "  # print('trimmed_count:', trimmed_count)\n",
        "  return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1_pSokR-msX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f0c2c016-02a5-4d05-c507-610c1e5796e7"
      },
      "source": [
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def prepare_dataset(inpput_df, batch_size, with_target=True):\n",
        "  questions = []\n",
        "  answers = []\n",
        "  output = []\n",
        "  data_q = torch.zeros([0, batch_size, MAX_TITLE_QUESTION_SIZE], dtype=torch.int64)\n",
        "  data_a = torch.zeros([0, batch_size, MAX_ANSWER_SIZE], dtype=torch.int64)\n",
        "  if with_target:\n",
        "    target = torch.zeros([0, batch_size, LABELS_COUNT], dtype=torch.float32)\n",
        "  for index, row in tqdm(inpput_df.iterrows()):\n",
        "    questions.append(row['question_title'] + ' ' + row['question_body'])\n",
        "    answers.append(row['answer'])\n",
        "    if with_target:\n",
        "      output.append(row[output_categories].values)\n",
        "    if len(questions) < batch_size:\n",
        "      continue\n",
        "    \n",
        "    questions_f = extract_features_for_batch(questions, MAX_TITLE_QUESTION_SIZE)\n",
        "    answers_f = extract_features_for_batch(answers, MAX_ANSWER_SIZE)\n",
        "    feat_q = questions_f.unsqueeze(0)\n",
        "    feat_a = answers_f.unsqueeze(0)\n",
        "\n",
        "    data_q = torch.cat((data_q, feat_q), 0)\n",
        "    data_a = torch.cat((data_a, feat_a), 0)\n",
        "    if with_target:\n",
        "      output = torch.Tensor(output).unsqueeze(0)\n",
        "      target = torch.cat((target, output), 0)\n",
        "\n",
        "    questions = []\n",
        "    answers = []\n",
        "    output = []\n",
        "  # TODO: extract the rest?\n",
        "\n",
        "  data_q = data_q.contiguous().to(device)\n",
        "  data_a = data_a.contiguous().to(device)\n",
        "  if with_target:\n",
        "    target = target.contiguous().to(device)\n",
        "  print('data_q:', data_q.shape)\n",
        "  print('data_a:', data_a.shape)\n",
        "  if with_target:\n",
        "    print('target:', target.shape)\n",
        "  if with_target:\n",
        "    return data_q, data_a, target\n",
        "  else:\n",
        "    return data_q, data_a\n",
        "\n",
        "train_q, train_a, train_target = prepare_dataset(df_train, TRAIN_BATCH_SIZE)\n",
        "test_q, test_a = prepare_dataset(df_test, EVAL_BATCH_SIZE, False)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100it [00:00, 151.17it/s]\n",
            "13it [00:00, 123.73it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data_q: torch.Size([25, 4, 160])\n",
            "data_a: torch.Size([25, 4, 128])\n",
            "target: torch.Size([25, 4, 30])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "476it [00:02, 162.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "data_q: torch.Size([476, 1, 160])\n",
            "data_a: torch.Size([476, 1, 128])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERAdg0ZSvTVk",
        "colab_type": "text"
      },
      "source": [
        "## Build pytorch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ABcdx2vUHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class BARTClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        inner_dim,\n",
        "        num_classes,\n",
        "        activation_fn,\n",
        "        pooler_dropout,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, inner_dim)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
        "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features\n",
        "        x = self.dropout(x)\n",
        "        # print('dense', x.shape)\n",
        "        x = self.dense(x)\n",
        "        # print('activation_fn', x.shape)\n",
        "        # print('x0:', x.cpu())\n",
        "        x = self.activation_fn(x)\n",
        "        # print('dropout', x.shape)\n",
        "        # print('x1:', x.cpu())\n",
        "        x = self.dropout(x)\n",
        "        # print('out_proj', x.shape)\n",
        "        x = self.out_proj(x)\n",
        "        # print('return', x.shape)\n",
        "        return x\n",
        "\n",
        "class BartMultiLabelModel(nn.Module):\n",
        "\n",
        "    def __init__(self, bart, q_size, a_size, bart_embedding_size, labels_count, dropout):\n",
        "        super(BartMultiLabelModel, self).__init__()\n",
        "        \n",
        "        self.bart = bart\n",
        "        self.dense = nn.Linear(q_size + a_size, 1)\n",
        "        inner_dimension = 512\n",
        "        self.head = BARTClassificationHead(\n",
        "            bart_embedding_size, inner_dimension,\n",
        "            labels_count, torch.tanh, dropout\n",
        "            )\n",
        "\n",
        "    def forward(self, src_q, src_a):\n",
        "        # print('src_q:', src_q.cpu().numpy())\n",
        "        # print('src_a:', src_a.cpu().numpy())\n",
        "        features_q = self.bart.extract_features(src_q)\n",
        "        features_a = self.bart.extract_features(src_a)\n",
        "        # print('features_q:', features_q.cpu())\n",
        "        # print('features_a:', features_a.cpu())\n",
        "        sys.stdout.flush()\n",
        "        features = torch.cat((features_q, features_a), 1)\n",
        "        # print('permute:', features.shape)\n",
        "        features = features.permute(0, 2, 1)\n",
        "        # print('dense:', features.shape)\n",
        "        # print('features0:', features.cpu())\n",
        "        features = self.dense(features).squeeze(2)\n",
        "        # print('head:', features.shape)\n",
        "        # print('features1:', features.cpu())\n",
        "        labels_logits = self.head(features)\n",
        "        # print('labels_logits:', labels_logits.cpu())\n",
        "        labels = torch.sigmoid(labels_logits)\n",
        "        return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjPAJ-aUMXHy",
        "colab_type": "text"
      },
      "source": [
        "## Initiate an instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gExjpGwMWeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = 0.2 # the dropout value\n",
        "model = BartMultiLabelModel(bart, MAX_TITLE_QUESTION_SIZE, MAX_ANSWER_SIZE, BART_EMBEDDING_SIZE, LABELS_COUNT, dropout).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kGPiU0qMFrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "18531b11-0bf4-4828-ee9d-fd35bdf6ae55"
      },
      "source": [
        "torch.cuda.memory_allocated() / 1024 / 1024 / 1024"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0438270568847656"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7uj08GfLBTc",
        "colab_type": "text"
      },
      "source": [
        "## Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP9H8O1u2RoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def compute_spearmanr_ignore_nan(trues, preds):\n",
        "    rhos = []\n",
        "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
        "        # print('tcol:', tcol, '\\npcol:', pcol)\n",
        "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
        "    return np.nanmean(rhos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SGw-1ko5Mwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "criterion = nn.BCELoss()\n",
        "lr = 2e-5 # learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train(train_q, train_a, train_target):\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    for i in range(0, train_q.size(0)):\n",
        "        question, answer, target = train_q[i], train_a[i], train_target[i]\n",
        "        optimizer.zero_grad()\n",
        "        print(f'step {i}')\n",
        "        # print(question.cpu())\n",
        "        # print(answer.cpu())\n",
        "        output = model(question, answer)\n",
        "        # print(target.cpu())\n",
        "        # print(output.cpu())\n",
        "        assert len(((target > 1.0) | (target < 0.0)).nonzero().cpu().numpy()) == 0\n",
        "        assert len(((output > 1.0) | (output < 0.0)).nonzero().cpu().numpy()) == 0\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 50\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.6f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, i, len(train_q), scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "dbg_output = None\n",
        "dbg_target = None\n",
        "\n",
        "test_result = []\n",
        "\n",
        "def evaluate(eval_model, valid_q, valid_a, valid_target, save_test):\n",
        "    global dbg_output\n",
        "    global dbg_target\n",
        "    global test_result\n",
        "\n",
        "    output_all = []\n",
        "    target_all = []\n",
        "    test_all = []\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, valid_q.size(0)):\n",
        "            question, answer, target = valid_q[i], valid_a[i], valid_target[i]\n",
        "            output = eval_model(question, answer)\n",
        "            output_all += output.cpu().numpy().tolist()\n",
        "            target_all += target.cpu().numpy().tolist()\n",
        "\n",
        "        dbg_output = output_all\n",
        "        dbg_target = target_all\n",
        "        total_loss = compute_spearmanr_ignore_nan(output_all, target_all)\n",
        "\n",
        "        if save_test:\n",
        "          for i in range(0, test_q.size(0)):\n",
        "              question, answer = test_q[i], test_a[i]\n",
        "              output = eval_model(question, answer)\n",
        "              test_all += output.cpu().numpy().tolist()\n",
        "          test_result.append(test_all)\n",
        "\n",
        "    return total_loss\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLHysEACXjpq",
        "colab_type": "text"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciFpBC7DXZSW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7c62743b-360a-4759-ef36-d04fef43a678"
      },
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "train_indexes = list(range(train_q.shape[0]))\n",
        "gkf = GroupKFold(n_splits=5).split(X=train_indexes, groups=train_indexes)\n",
        "\n",
        "best_val_loss = 0\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "valid_preds = []\n",
        "test_preds = []\n",
        "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
        "\n",
        "  if fold in [0, 2]:\n",
        "    print(f'fold: {fold}')\n",
        "    train_input_q, train_input_a, train_input_t = train_q[train_idx], train_a[train_idx], train_target[train_idx]\n",
        "    valid_input_q, valid_input_a, valid_input_t = train_q[valid_idx], train_a[valid_idx], train_target[valid_idx]\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        epoch_start_time = time.time()\n",
        "        train(train_input_q, train_input_a, train_input_t)\n",
        "        val_loss = evaluate(model, valid_input_q, valid_input_a, valid_input_t, epoch == 3)\n",
        "        print('-' * 89)\n",
        "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "              'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                        val_loss, math.exp(val_loss)))\n",
        "        print('-' * 89)\n",
        "\n",
        "        if val_loss > best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = model\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    torch.save(model.state_dict(), OUTPUT_PATH + f'fold_{fold}.cpt')"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold: 0\n",
            "step 0\n",
            "step 1\n",
            "step 2\n",
            "step 3\n",
            "step 4\n",
            "step 5\n",
            "step 6\n",
            "step 7\n",
            "step 8\n",
            "step 9\n",
            "step 10\n",
            "step 11\n",
            "step 12\n",
            "step 13\n",
            "step 14\n",
            "step 15\n",
            "step 16\n",
            "step 17\n",
            "step 18\n",
            "step 19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2534: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[:, None]\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:2535: RuntimeWarning: invalid value encountered in true_divide\n",
            "  c /= stddev[None, :]\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in greater\n",
            "  return (a < x) & (x < b)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:903: RuntimeWarning: invalid value encountered in less\n",
            "  return (a < x) & (x < b)\n",
            "/usr/local/lib/python3.6/dist-packages/scipy/stats/_distn_infrastructure.py:1912: RuntimeWarning: invalid value encountered in less_equal\n",
            "  cond2 = cond0 & (x <= _a)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  6.92s | valid loss  0.05 | valid ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "step 0\n",
            "step 1\n",
            "step 2\n",
            "step 3\n",
            "step 4\n",
            "step 5\n",
            "step 6\n",
            "step 7\n",
            "step 8\n",
            "step 9\n",
            "step 10\n",
            "step 11\n",
            "step 12\n",
            "step 13\n",
            "step 14\n",
            "step 15\n",
            "step 16\n",
            "step 17\n",
            "step 18\n",
            "step 19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  6.90s | valid loss  0.06 | valid ppl     1.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "step 0\n",
            "step 1\n",
            "step 2\n",
            "step 3\n",
            "step 4\n",
            "step 5\n",
            "step 6\n",
            "step 7\n",
            "step 8\n",
            "step 9\n",
            "step 10\n",
            "step 11\n",
            "step 12\n",
            "step 13\n",
            "step 14\n",
            "step 15\n",
            "step 16\n",
            "step 17\n",
            "step 18\n",
            "step 19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 48.25s | valid loss  0.07 | valid ppl     1.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "fold: 2\n",
            "step 0\n",
            "step 1\n",
            "step 2\n",
            "step 3\n",
            "step 4\n",
            "step 5\n",
            "step 6\n",
            "step 7\n",
            "step 8\n",
            "step 9\n",
            "step 10\n",
            "step 11\n",
            "step 12\n",
            "step 13\n",
            "step 14\n",
            "step 15\n",
            "step 16\n",
            "step 17\n",
            "step 18\n",
            "step 19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  6.99s | valid loss  0.22 | valid ppl     1.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "step 0\n",
            "step 1\n",
            "step 2\n",
            "step 3\n",
            "step 4\n",
            "step 5\n",
            "step 6\n",
            "step 7\n",
            "step 8\n",
            "step 9\n",
            "step 10\n",
            "step 11\n",
            "step 12\n",
            "step 13\n",
            "step 14\n",
            "step 15\n",
            "step 16\n",
            "step 17\n",
            "step 18\n",
            "step 19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  7.03s | valid loss  0.21 | valid ppl     1.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "step 0\n",
            "step 1\n",
            "step 2\n",
            "step 3\n",
            "step 4\n",
            "step 5\n",
            "step 6\n",
            "step 7\n",
            "step 8\n",
            "step 9\n",
            "step 10\n",
            "step 11\n",
            "step 12\n",
            "step 13\n",
            "step 14\n",
            "step 15\n",
            "step 16\n",
            "step 17\n",
            "step 18\n",
            "step 19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 47.90s | valid loss  0.21 | valid ppl     1.23\n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSsARwOLQqNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sub.iloc[:, 1:] = np.average(test_result, axis=0) # for weighted average set weights=[...]\n",
        "\n",
        "df_sub.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0_FtVchHp2N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "dfb06431-79bf-449a-d2bd-dcdd0b993de4"
      },
      "source": [
        "df_sub"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qa_id</th>\n",
              "      <th>question_asker_intent_understanding</th>\n",
              "      <th>question_body_critical</th>\n",
              "      <th>question_conversational</th>\n",
              "      <th>question_expect_short_answer</th>\n",
              "      <th>question_fact_seeking</th>\n",
              "      <th>question_has_commonly_accepted_answer</th>\n",
              "      <th>question_interestingness_others</th>\n",
              "      <th>question_interestingness_self</th>\n",
              "      <th>question_multi_intent</th>\n",
              "      <th>question_not_really_a_question</th>\n",
              "      <th>question_opinion_seeking</th>\n",
              "      <th>question_type_choice</th>\n",
              "      <th>question_type_compare</th>\n",
              "      <th>question_type_consequence</th>\n",
              "      <th>question_type_definition</th>\n",
              "      <th>question_type_entity</th>\n",
              "      <th>question_type_instructions</th>\n",
              "      <th>question_type_procedure</th>\n",
              "      <th>question_type_reason_explanation</th>\n",
              "      <th>question_type_spelling</th>\n",
              "      <th>question_well_written</th>\n",
              "      <th>answer_helpful</th>\n",
              "      <th>answer_level_of_information</th>\n",
              "      <th>answer_plausible</th>\n",
              "      <th>answer_relevance</th>\n",
              "      <th>answer_satisfaction</th>\n",
              "      <th>answer_type_instructions</th>\n",
              "      <th>answer_type_procedure</th>\n",
              "      <th>answer_type_reason_explanation</th>\n",
              "      <th>answer_well_written</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>39</td>\n",
              "      <td>0.877141</td>\n",
              "      <td>0.616454</td>\n",
              "      <td>0.100015</td>\n",
              "      <td>0.667125</td>\n",
              "      <td>0.774096</td>\n",
              "      <td>0.816812</td>\n",
              "      <td>0.644253</td>\n",
              "      <td>0.563869</td>\n",
              "      <td>0.335359</td>\n",
              "      <td>0.074340</td>\n",
              "      <td>0.412480</td>\n",
              "      <td>0.357989</td>\n",
              "      <td>0.157653</td>\n",
              "      <td>0.088387</td>\n",
              "      <td>0.101443</td>\n",
              "      <td>0.122822</td>\n",
              "      <td>0.573285</td>\n",
              "      <td>0.152796</td>\n",
              "      <td>0.361950</td>\n",
              "      <td>0.066607</td>\n",
              "      <td>0.792064</td>\n",
              "      <td>0.851314</td>\n",
              "      <td>0.591471</td>\n",
              "      <td>0.886523</td>\n",
              "      <td>0.893828</td>\n",
              "      <td>0.804920</td>\n",
              "      <td>0.548351</td>\n",
              "      <td>0.169914</td>\n",
              "      <td>0.519769</td>\n",
              "      <td>0.850138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>46</td>\n",
              "      <td>0.898955</td>\n",
              "      <td>0.611716</td>\n",
              "      <td>0.079084</td>\n",
              "      <td>0.671899</td>\n",
              "      <td>0.793530</td>\n",
              "      <td>0.808872</td>\n",
              "      <td>0.590139</td>\n",
              "      <td>0.478742</td>\n",
              "      <td>0.271141</td>\n",
              "      <td>0.054252</td>\n",
              "      <td>0.399471</td>\n",
              "      <td>0.305475</td>\n",
              "      <td>0.085915</td>\n",
              "      <td>0.069423</td>\n",
              "      <td>0.070037</td>\n",
              "      <td>0.094043</td>\n",
              "      <td>0.654783</td>\n",
              "      <td>0.148608</td>\n",
              "      <td>0.344156</td>\n",
              "      <td>0.050996</td>\n",
              "      <td>0.823982</td>\n",
              "      <td>0.918951</td>\n",
              "      <td>0.651311</td>\n",
              "      <td>0.901699</td>\n",
              "      <td>0.922459</td>\n",
              "      <td>0.814832</td>\n",
              "      <td>0.570551</td>\n",
              "      <td>0.134482</td>\n",
              "      <td>0.510315</td>\n",
              "      <td>0.907683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70</td>\n",
              "      <td>0.833094</td>\n",
              "      <td>0.593902</td>\n",
              "      <td>0.106251</td>\n",
              "      <td>0.626903</td>\n",
              "      <td>0.798549</td>\n",
              "      <td>0.825583</td>\n",
              "      <td>0.633340</td>\n",
              "      <td>0.523447</td>\n",
              "      <td>0.300101</td>\n",
              "      <td>0.086720</td>\n",
              "      <td>0.410109</td>\n",
              "      <td>0.302424</td>\n",
              "      <td>0.179193</td>\n",
              "      <td>0.099688</td>\n",
              "      <td>0.133514</td>\n",
              "      <td>0.141509</td>\n",
              "      <td>0.532668</td>\n",
              "      <td>0.178335</td>\n",
              "      <td>0.399814</td>\n",
              "      <td>0.081010</td>\n",
              "      <td>0.811036</td>\n",
              "      <td>0.850650</td>\n",
              "      <td>0.587747</td>\n",
              "      <td>0.862383</td>\n",
              "      <td>0.869346</td>\n",
              "      <td>0.802334</td>\n",
              "      <td>0.530934</td>\n",
              "      <td>0.187423</td>\n",
              "      <td>0.512009</td>\n",
              "      <td>0.840998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>132</td>\n",
              "      <td>0.863999</td>\n",
              "      <td>0.562678</td>\n",
              "      <td>0.093936</td>\n",
              "      <td>0.606455</td>\n",
              "      <td>0.829432</td>\n",
              "      <td>0.831525</td>\n",
              "      <td>0.622698</td>\n",
              "      <td>0.494134</td>\n",
              "      <td>0.218495</td>\n",
              "      <td>0.081221</td>\n",
              "      <td>0.414254</td>\n",
              "      <td>0.295160</td>\n",
              "      <td>0.143180</td>\n",
              "      <td>0.101919</td>\n",
              "      <td>0.087600</td>\n",
              "      <td>0.157236</td>\n",
              "      <td>0.628044</td>\n",
              "      <td>0.150202</td>\n",
              "      <td>0.309018</td>\n",
              "      <td>0.080979</td>\n",
              "      <td>0.812302</td>\n",
              "      <td>0.896958</td>\n",
              "      <td>0.661133</td>\n",
              "      <td>0.842160</td>\n",
              "      <td>0.878900</td>\n",
              "      <td>0.813574</td>\n",
              "      <td>0.512168</td>\n",
              "      <td>0.171499</td>\n",
              "      <td>0.487258</td>\n",
              "      <td>0.888581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>200</td>\n",
              "      <td>0.900336</td>\n",
              "      <td>0.600945</td>\n",
              "      <td>0.121769</td>\n",
              "      <td>0.738044</td>\n",
              "      <td>0.777405</td>\n",
              "      <td>0.783939</td>\n",
              "      <td>0.707085</td>\n",
              "      <td>0.537361</td>\n",
              "      <td>0.350141</td>\n",
              "      <td>0.086716</td>\n",
              "      <td>0.471242</td>\n",
              "      <td>0.346732</td>\n",
              "      <td>0.112167</td>\n",
              "      <td>0.085320</td>\n",
              "      <td>0.105621</td>\n",
              "      <td>0.119446</td>\n",
              "      <td>0.546479</td>\n",
              "      <td>0.158188</td>\n",
              "      <td>0.359814</td>\n",
              "      <td>0.042938</td>\n",
              "      <td>0.787178</td>\n",
              "      <td>0.891369</td>\n",
              "      <td>0.643028</td>\n",
              "      <td>0.906658</td>\n",
              "      <td>0.899718</td>\n",
              "      <td>0.866897</td>\n",
              "      <td>0.554831</td>\n",
              "      <td>0.109364</td>\n",
              "      <td>0.583250</td>\n",
              "      <td>0.906128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>471</th>\n",
              "      <td>9569</td>\n",
              "      <td>0.877681</td>\n",
              "      <td>0.616237</td>\n",
              "      <td>0.082556</td>\n",
              "      <td>0.670140</td>\n",
              "      <td>0.797347</td>\n",
              "      <td>0.823271</td>\n",
              "      <td>0.651169</td>\n",
              "      <td>0.479586</td>\n",
              "      <td>0.257198</td>\n",
              "      <td>0.065010</td>\n",
              "      <td>0.447204</td>\n",
              "      <td>0.335242</td>\n",
              "      <td>0.110328</td>\n",
              "      <td>0.083033</td>\n",
              "      <td>0.103441</td>\n",
              "      <td>0.125087</td>\n",
              "      <td>0.612523</td>\n",
              "      <td>0.120701</td>\n",
              "      <td>0.322327</td>\n",
              "      <td>0.069469</td>\n",
              "      <td>0.767738</td>\n",
              "      <td>0.913869</td>\n",
              "      <td>0.654693</td>\n",
              "      <td>0.892823</td>\n",
              "      <td>0.907519</td>\n",
              "      <td>0.806629</td>\n",
              "      <td>0.544480</td>\n",
              "      <td>0.147102</td>\n",
              "      <td>0.483149</td>\n",
              "      <td>0.893290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>9590</td>\n",
              "      <td>0.891896</td>\n",
              "      <td>0.588887</td>\n",
              "      <td>0.089983</td>\n",
              "      <td>0.677096</td>\n",
              "      <td>0.787019</td>\n",
              "      <td>0.816666</td>\n",
              "      <td>0.644676</td>\n",
              "      <td>0.499943</td>\n",
              "      <td>0.227593</td>\n",
              "      <td>0.070638</td>\n",
              "      <td>0.385118</td>\n",
              "      <td>0.362580</td>\n",
              "      <td>0.100108</td>\n",
              "      <td>0.074729</td>\n",
              "      <td>0.088268</td>\n",
              "      <td>0.120974</td>\n",
              "      <td>0.604968</td>\n",
              "      <td>0.170800</td>\n",
              "      <td>0.344929</td>\n",
              "      <td>0.071283</td>\n",
              "      <td>0.808807</td>\n",
              "      <td>0.880603</td>\n",
              "      <td>0.664532</td>\n",
              "      <td>0.906795</td>\n",
              "      <td>0.894837</td>\n",
              "      <td>0.817518</td>\n",
              "      <td>0.571075</td>\n",
              "      <td>0.155758</td>\n",
              "      <td>0.530231</td>\n",
              "      <td>0.902892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473</th>\n",
              "      <td>9597</td>\n",
              "      <td>0.884911</td>\n",
              "      <td>0.538689</td>\n",
              "      <td>0.081323</td>\n",
              "      <td>0.728951</td>\n",
              "      <td>0.734760</td>\n",
              "      <td>0.826563</td>\n",
              "      <td>0.578927</td>\n",
              "      <td>0.488141</td>\n",
              "      <td>0.254923</td>\n",
              "      <td>0.065422</td>\n",
              "      <td>0.412881</td>\n",
              "      <td>0.316049</td>\n",
              "      <td>0.090607</td>\n",
              "      <td>0.060003</td>\n",
              "      <td>0.115508</td>\n",
              "      <td>0.095732</td>\n",
              "      <td>0.635653</td>\n",
              "      <td>0.175357</td>\n",
              "      <td>0.350809</td>\n",
              "      <td>0.061654</td>\n",
              "      <td>0.759858</td>\n",
              "      <td>0.897303</td>\n",
              "      <td>0.638445</td>\n",
              "      <td>0.907377</td>\n",
              "      <td>0.913706</td>\n",
              "      <td>0.800652</td>\n",
              "      <td>0.631786</td>\n",
              "      <td>0.124080</td>\n",
              "      <td>0.524823</td>\n",
              "      <td>0.896961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>474</th>\n",
              "      <td>9623</td>\n",
              "      <td>0.868056</td>\n",
              "      <td>0.516241</td>\n",
              "      <td>0.088748</td>\n",
              "      <td>0.707393</td>\n",
              "      <td>0.745510</td>\n",
              "      <td>0.754734</td>\n",
              "      <td>0.685680</td>\n",
              "      <td>0.487921</td>\n",
              "      <td>0.258393</td>\n",
              "      <td>0.112859</td>\n",
              "      <td>0.419127</td>\n",
              "      <td>0.326185</td>\n",
              "      <td>0.109189</td>\n",
              "      <td>0.059272</td>\n",
              "      <td>0.118426</td>\n",
              "      <td>0.124403</td>\n",
              "      <td>0.537756</td>\n",
              "      <td>0.189677</td>\n",
              "      <td>0.327902</td>\n",
              "      <td>0.054828</td>\n",
              "      <td>0.825422</td>\n",
              "      <td>0.905576</td>\n",
              "      <td>0.619418</td>\n",
              "      <td>0.866297</td>\n",
              "      <td>0.902096</td>\n",
              "      <td>0.773388</td>\n",
              "      <td>0.514666</td>\n",
              "      <td>0.141671</td>\n",
              "      <td>0.610821</td>\n",
              "      <td>0.866399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>9640</td>\n",
              "      <td>0.876215</td>\n",
              "      <td>0.553181</td>\n",
              "      <td>0.078176</td>\n",
              "      <td>0.582929</td>\n",
              "      <td>0.802820</td>\n",
              "      <td>0.818177</td>\n",
              "      <td>0.632607</td>\n",
              "      <td>0.514993</td>\n",
              "      <td>0.319521</td>\n",
              "      <td>0.061412</td>\n",
              "      <td>0.431961</td>\n",
              "      <td>0.262588</td>\n",
              "      <td>0.107672</td>\n",
              "      <td>0.092059</td>\n",
              "      <td>0.091817</td>\n",
              "      <td>0.126915</td>\n",
              "      <td>0.602496</td>\n",
              "      <td>0.141789</td>\n",
              "      <td>0.341004</td>\n",
              "      <td>0.069681</td>\n",
              "      <td>0.805336</td>\n",
              "      <td>0.879752</td>\n",
              "      <td>0.679674</td>\n",
              "      <td>0.893455</td>\n",
              "      <td>0.908800</td>\n",
              "      <td>0.834178</td>\n",
              "      <td>0.521398</td>\n",
              "      <td>0.141284</td>\n",
              "      <td>0.481770</td>\n",
              "      <td>0.872552</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>476 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     qa_id  ...  answer_well_written\n",
              "0       39  ...             0.850138\n",
              "1       46  ...             0.907683\n",
              "2       70  ...             0.840998\n",
              "3      132  ...             0.888581\n",
              "4      200  ...             0.906128\n",
              "..     ...  ...                  ...\n",
              "471   9569  ...             0.893290\n",
              "472   9590  ...             0.902892\n",
              "473   9597  ...             0.896961\n",
              "474   9623  ...             0.866399\n",
              "475   9640  ...             0.872552\n",
              "\n",
              "[476 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhQTKVlMHueD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}