{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9m926P/bOamjdtuWBV6No"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2KrULHs1O6S",
        "colab_type": "code",
        "outputId": "53d10b4e-10d9-41f3-e981-ec96c02678b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL76V1uI5AdH",
        "colab_type": "code",
        "outputId": "6a71d9f1-236f-4692-aaea-96d5d9b9047f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Feb  9 20:40:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkow5j7d5JRy",
        "colab_type": "code",
        "outputId": "fc7aec35-6fcb-4765-8b1b-b25e026dab0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImxqguB_5S9o",
        "colab_type": "code",
        "outputId": "6edba397-d603-4eed-abb9-43a0aa6b904f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bart = torch.hub.load('pytorch/fairseq', 'bart.large')\n",
        "bart.eval()  # disable dropout (or leave in train mode to finetune)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIuuaIKbfUNK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for p in bart.parameters():\n",
        "  p.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QZ2fNu-9JGY",
        "colab_type": "text"
      },
      "source": [
        "## Convert whole dataset to bart embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsVSBsw-9gLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "PATH = '/content/drive/My Drive/Kaggle/Google QUEST Q&A Labeling/data/base/'\n",
        "OUTPUT_PATH = '/content/drive/My Drive/Kaggle/Google QUEST Q&A Labeling/output/2/'\n",
        "\n",
        "df_train = pd.read_csv(PATH+'train.csv')\n",
        "df_test = pd.read_csv(PATH+'test.csv')\n",
        "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
        "print('train shape =', df_train.shape)\n",
        "print('test shape =', df_test.shape)\n",
        "\n",
        "output_categories = list(df_train.columns[11:])\n",
        "input_categories = list(df_train.columns[[1,2,5]])\n",
        "print('\\noutput categories:\\n\\t', output_categories)\n",
        "print('\\ninput categories:\\n\\t', input_categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDsZcDq-9jSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An90Lahsd9V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train[df_train[output_categories] > 1.0].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BOjamzARAno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXHHvitT9qDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN_BATCH_SIZE = 4\n",
        "EVAL_BATCH_SIZE = 4\n",
        "# MAX_TITLE_SIZE = 64\n",
        "MAX_TITLE_SIZE = 32\n",
        "# MAX_QUESTION_SIZE = 512\n",
        "MAX_QUESTION_SIZE = 256\n",
        "# MAX_ANSWER_SIZE = 512\n",
        "MAX_ANSWER_SIZE = 256\n",
        "BART_EMBEDDING_SIZE = 1024\n",
        "MAX_TITLE_QUESTION_SIZE = MAX_TITLE_SIZE + MAX_QUESTION_SIZE\n",
        "LABELS_COUNT = len(output_categories)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPkvwKF0FZJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fairseq.data.data_utils import collate_tokens\n",
        "\n",
        "def extract_features_for_batch(batch_snt, max_length):\n",
        "  batch_encoded = []\n",
        "  trimmed_count = 0\n",
        "  for s in batch_snt:\n",
        "    encoded = bart.encode(s)\n",
        "    if len(encoded) <= max_length:\n",
        "      encoded = torch.cat((encoded, torch.tensor([1]).repeat(max_length - len(encoded))), 0)\n",
        "    else:\n",
        "      encoded = encoded[:max_length]\n",
        "      trimmed_count += 1\n",
        "    batch_encoded.append(encoded)\n",
        "  batch = collate_tokens(batch_encoded, pad_idx=1)\n",
        "  # print('trimmed_count:', trimmed_count)\n",
        "  return batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1_pSokR-msX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def prepare_dataset(inpput_df, batch_size):\n",
        "  titles = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  output = []\n",
        "  data_q = torch.zeros([0, batch_size, MAX_TITLE_QUESTION_SIZE], dtype=torch.int64)\n",
        "  data_a = torch.zeros([0, batch_size, MAX_ANSWER_SIZE], dtype=torch.int64)\n",
        "  target = torch.zeros([0, batch_size, LABELS_COUNT], dtype=torch.float32)\n",
        "  for index, row in tqdm(inpput_df.iterrows()):\n",
        "    titles.append(row['question_title'])\n",
        "    questions.append(row['question_body'])\n",
        "    answers.append(row['answer'])\n",
        "    output.append(row[output_categories].values)\n",
        "    if len(titles) < batch_size:\n",
        "      continue\n",
        "    \n",
        "    titles_f = extract_features_for_batch(titles, MAX_TITLE_SIZE)\n",
        "    questions_f = extract_features_for_batch(questions, MAX_QUESTION_SIZE)\n",
        "    answers_f = extract_features_for_batch(answers, MAX_ANSWER_SIZE)\n",
        "    feat_q = torch.cat((titles_f, questions_f), 1).unsqueeze(0)\n",
        "    feat_a = answers_f.unsqueeze(0)\n",
        "\n",
        "    data_q = torch.cat((data_q, feat_q), 0)\n",
        "    data_a = torch.cat((data_a, feat_a), 0)\n",
        "    output = torch.Tensor(output).unsqueeze(0)\n",
        "    target = torch.cat((target, output), 0)\n",
        "\n",
        "    titles = []\n",
        "    questions = []\n",
        "    answers = []\n",
        "    output = []\n",
        "  # TODO: extract the rest?\n",
        "\n",
        "  data_q = data_q.contiguous().to(device)\n",
        "  data_a = data_a.contiguous().to(device)\n",
        "  target = target.contiguous().to(device)\n",
        "  print('data_q:', data_q.shape)\n",
        "  print('data_a:', data_a.shape)\n",
        "  print('target:', target.shape)\n",
        "  return data_q, data_a, target\n",
        "\n",
        "train_q, train_a, train_target = prepare_dataset(df_train, TRAIN_BATCH_SIZE)\n",
        "# test_q, test_a, test_target = prepare_dataset(df_test, EVAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERAdg0ZSvTVk",
        "colab_type": "text"
      },
      "source": [
        "## Build pytorch model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ABcdx2vUHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class BARTClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_dim,\n",
        "        inner_dim,\n",
        "        num_classes,\n",
        "        activation_fn,\n",
        "        pooler_dropout,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, inner_dim)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.dropout = nn.Dropout(p=pooler_dropout)\n",
        "        self.out_proj = nn.Linear(inner_dim, num_classes)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features\n",
        "        x = self.dropout(x)\n",
        "        # print('dense', x.shape)\n",
        "        x = self.dense(x)\n",
        "        # print('activation_fn', x.shape)\n",
        "        print('x0:', x.cpu())\n",
        "        x = self.activation_fn(x)\n",
        "        # print('dropout', x.shape)\n",
        "        print('x1:', x.cpu())\n",
        "        x = self.dropout(x)\n",
        "        # print('out_proj', x.shape)\n",
        "        x = self.out_proj(x)\n",
        "        # print('return', x.shape)\n",
        "        return x\n",
        "\n",
        "class BartMultiLabelModel(nn.Module):\n",
        "\n",
        "    def __init__(self, bart, q_size, a_size, bart_embedding_size, labels_count, dropout):\n",
        "        super(BartMultiLabelModel, self).__init__()\n",
        "        \n",
        "        self.bart = bart\n",
        "        self.dense = nn.Linear(q_size + a_size, 1)\n",
        "        inner_dimension = 512\n",
        "        self.head = BARTClassificationHead(\n",
        "            bart_embedding_size, inner_dimension,\n",
        "            labels_count, torch.tanh, dropout\n",
        "            )\n",
        "\n",
        "    def forward(self, src_q, src_a):\n",
        "        print('src_q:', src_q.cpu().numpy())\n",
        "        print('src_a:', src_a.cpu().numpy())\n",
        "        features_q = self.bart.extract_features(src_q)\n",
        "        features_a = self.bart.extract_features(src_a)\n",
        "        if (features_q != features_q).any():\n",
        "          for i, a in enumerate(src_q):\n",
        "            for j, b in enumerate(a):\n",
        "              print(f'[{i}, {j}]: {b}')\n",
        "        print('features_q:', features_q.cpu())\n",
        "        print('features_a:', features_a.cpu())\n",
        "        sys.stdout.flush()\n",
        "        features = torch.cat((features_q, features_a), 1)\n",
        "        # print('permute:', features.shape)\n",
        "        features = features.permute(0, 2, 1)\n",
        "        # print('dense:', features.shape)\n",
        "        print('features0:', features.cpu())\n",
        "        features = self.dense(features).squeeze(2)\n",
        "        # print('head:', features.shape)\n",
        "        print('features1:', features.cpu())\n",
        "        labels_logits = self.head(features)\n",
        "        print('labels_logits:', labels_logits.cpu())\n",
        "        labels = torch.sigmoid(labels_logits)\n",
        "        return labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjPAJ-aUMXHy",
        "colab_type": "text"
      },
      "source": [
        "## Initiate an instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gExjpGwMWeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout = 0.2 # the dropout value\n",
        "model = BartMultiLabelModel(bart, MAX_TITLE_QUESTION_SIZE, MAX_ANSWER_SIZE, BART_EMBEDDING_SIZE, LABELS_COUNT, dropout).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kGPiU0qMFrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.memory_allocated() / 1024 / 1024 / 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7uj08GfLBTc",
        "colab_type": "text"
      },
      "source": [
        "## Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SGw-1ko5Mwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "lr = 2e-5 # learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    for i in range(0, train_q.size(0) - 1):\n",
        "        question, answer, target = train_q[i], train_a[i], train_target[i]\n",
        "        optimizer.zero_grad()\n",
        "        print(f'step {i}')\n",
        "        print(question.cpu())\n",
        "        print(answer.cpu())\n",
        "        output = model(question, answer)\n",
        "        print(target.cpu())\n",
        "        print(output.cpu())\n",
        "        assert len(((target > 1.0) | (target < 0.0)).nonzero().cpu().numpy()) == 0\n",
        "        assert len(((output > 1.0) | (output < 0.0)).nonzero().cpu().numpy()) == 0\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 50\n",
        "        if i % log_interval == 0 and i > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, i, len(train_q), scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, test_q.size(0) - 1):\n",
        "            question, answer, target = test_q[i], test_q[i], test_target[i]\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLHysEACXjpq",
        "colab_type": "text"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciFpBC7DXZSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training():\n",
        "  best_val_loss = float(\"inf\")\n",
        "  epochs = 3 # The number of epochs\n",
        "  best_model = None\n",
        "\n",
        "  for epoch in range(1, epochs + 1):\n",
        "      epoch_start_time = time.time()\n",
        "      train()\n",
        "      val_loss = evaluate(model, val_data)\n",
        "      print('-' * 89)\n",
        "      print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "            'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                      val_loss, math.exp(val_loss)))\n",
        "      print('-' * 89)\n",
        "\n",
        "      if val_loss < best_val_loss:\n",
        "          best_val_loss = val_loss\n",
        "          best_model = model\n",
        "\n",
        "      scheduler.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OftPNiiDp9-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from multiprocessing import Process, Queue\n",
        "\n",
        "queue = Queue()\n",
        "p = Process(target=run_training, args=())\n",
        "p.start()\n",
        "p.join() # this blocks until the process terminates\n",
        "result = queue.get()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSsARwOLQqNp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.Tensor([1, 2, 3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDG-XBLKmumw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "((a == 5) | (a == 3)).nonzero().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5vWiwBHN_qo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VCxsDRqOKkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(a != 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g11GZYVgOO4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}